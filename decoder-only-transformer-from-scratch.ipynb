{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pip\n\ntry:\n    __import__('lightning')\nexcept:\n    pip.main(['install', 'lightning'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport lightning as L","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:02.795913Z","iopub.execute_input":"2025-06-24T12:24:02.796194Z","iopub.status.idle":"2025-06-24T12:24:02.800465Z","shell.execute_reply.started":"2025-06-24T12:24:02.796176Z","shell.execute_reply":"2025-06-24T12:24:02.799553Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# \"What you cannot create you do not understand.\"\n# â€” Richard Feynman","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:02.970216Z","iopub.execute_input":"2025-06-24T12:24:02.970813Z","iopub.status.idle":"2025-06-24T12:24:02.973873Z","shell.execute_reply.started":"2025-06-24T12:24:02.970791Z","shell.execute_reply":"2025-06-24T12:24:02.973079Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntoken_to_id = {\n    'what': 0,\n    'you': 1,\n    'cannot': 2,\n    'create': 3,\n    'do': 4,\n    'not': 5,\n    'understand': 6,\n    '<EOS>': 7,\n}\n\nid_to_token = dict(map(reversed, token_to_id.items()))\n\n# Tokenized sentence with <EOS>\nfull_sequence = [token_to_id[tok] for tok in ['what', 'you', 'cannot', 'create', 'you', 'do', 'not', 'understand', '<EOS>']]\n\n# Split into input-label pairs using shifting\ninputs = torch.tensor([full_sequence[:-1]])   # all tokens except the last\nlabels = torch.tensor([full_sequence[1:]])    # all tokens except the first\n\n# Create dataset and dataloader\ndataset = TensorDataset(inputs, labels)\ndataloader = DataLoader(dataset, batch_size=1)\n\n# For demonstration, let's print\nfor batch in dataloader:\n    x, y = batch\n    print(\"Inputs: \", x)\n    print(\"Labels: \", y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.125557Z","iopub.execute_input":"2025-06-24T12:24:03.125793Z","iopub.status.idle":"2025-06-24T12:24:03.134197Z","shell.execute_reply.started":"2025-06-24T12:24:03.125775Z","shell.execute_reply":"2025-06-24T12:24:03.133365Z"}},"outputs":[{"name":"stdout","text":"Inputs:  tensor([[0, 1, 2, 3, 1, 4, 5, 6]])\nLabels:  tensor([[1, 2, 3, 1, 4, 5, 6, 7]])\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model=2, max_len=6):\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model) # positional enconding\n\n        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n\n        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n\n        pe[:, 0::2] = torch.sin(position * div_term) ## every other column, starting with the 1st, has sin() values\n        pe[:, 1::2] = torch.cos(position * div_term) ## every other column, starting with the 2nd, has cos() values\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, word_embeddings):\n        return word_embeddings + self.pe[:word_embeddings.size(0)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.283567Z","iopub.execute_input":"2025-06-24T12:24:03.284252Z","iopub.status.idle":"2025-06-24T12:24:03.289955Z","shell.execute_reply.started":"2025-06-24T12:24:03.284222Z","shell.execute_reply":"2025-06-24T12:24:03.289287Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, d_model=2):\n        super().__init__()\n\n        self.d_model = d_model\n\n        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n\n        self.row_dim = 0\n        self.col_dim = 1\n\n    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask):\n        q = self.W_q(encodings_for_q)\n        k = self.W_k(encodings_for_k)\n        v = self.W_v(encodings_for_v)\n\n        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n\n        # if mask is not None:\n        scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n\n        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n        attention_scores = torch.matmul(attention_percents, v)\n        \n        return attention_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.478081Z","iopub.execute_input":"2025-06-24T12:24:03.478328Z","iopub.status.idle":"2025-06-24T12:24:03.484132Z","shell.execute_reply.started":"2025-06-24T12:24:03.478309Z","shell.execute_reply":"2025-06-24T12:24:03.483490Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class DecoderOnlyTransformer(L.LightningModule):\n    \n    def __init__(self, num_tokens=4, d_model=2, max_len=7):\n        \n        super().__init__()\n        \n        # L.seed_everything(seed=42)\n        \n        self.we = nn.Embedding(num_embeddings=num_tokens, \n                               embedding_dim=d_model)     \n        \n        self.pe = PositionalEncoding(d_model=d_model, \n                                   max_len=max_len)\n\n        self.self_attention = Attention(d_model=d_model)\n        self.self_attention_2 = Attention(d_model=d_model)\n        self.self_attention_3 = Attention(d_model=d_model)\n        self.self_attention_4 = Attention(d_model=d_model)\n\n        self.reduce_attention_dim = nn.Linear(in_features=(4*d_model), out_features=d_model)\n\n        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n        \n        self.loss = nn.CrossEntropyLoss()\n        # self.loss = F.softmax()\n        \n        \n    def forward(self, token_ids):\n                \n        word_embeddings = self.we(token_ids)        \n        position_encoded = self.pe(word_embeddings)\n        \n        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n        mask = mask == 0\n\n        self_attention_values = self.self_attention(position_encoded, \n                                                    position_encoded, \n                                                    position_encoded, \n                                                    mask=mask)\n                \n        residual_connection_values = position_encoded + self_attention_values\n        \n        fc_layer_output = self.fc_layer(residual_connection_values)\n        \n        return fc_layer_output\n    \n    \n    def configure_optimizers(self): \n        return Adam(self.parameters(), lr=0.1)\n    \n    \n    def training_step(self, batch, batch_idx):\n        input_tokens, labels = batch # collect input\n        output = self.forward(input_tokens[0])\n        loss = self.loss(output, labels[0])\n        print(f'Loss: {loss}')\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.600939Z","iopub.execute_input":"2025-06-24T12:24:03.601367Z","iopub.status.idle":"2025-06-24T12:24:03.608810Z","shell.execute_reply.started":"2025-06-24T12:24:03.601348Z","shell.execute_reply":"2025-06-24T12:24:03.608108Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"## First, create a model from DecoderOnlyTransformer()\nmodel = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=12)\n\n## Now create the input for the transformer...\nmodel_input = torch.tensor([token_to_id[\"what\"], \n                           #  token_to_id[\"is\"], \n                           #  token_to_id[\"statquest\"], \n                           #  token_to_id[\"<EOS>\"]\n                           ])\ninput_length = model_input.size(dim=0)\n\n## Now get get predictions from the model\npredictions = model(model_input) \n\npredicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n\npredicted_ids = predicted_id\n\n\nmax_length = 7\nfor i in range(input_length, max_length):\n    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n        break\n    \n    model_input = torch.cat((model_input, predicted_id))\n    \n    predictions = model(model_input) \n    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n    predicted_ids = torch.cat((predicted_ids, predicted_id))\n        \n## Now printout the predicted output phrase.\nprint(\"Predicted Tokens:\\n\") \nfor id in predicted_ids: \n    print(\"\\t\", id_to_token[id.item()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.776332Z","iopub.execute_input":"2025-06-24T12:24:03.776845Z","iopub.status.idle":"2025-06-24T12:24:03.789257Z","shell.execute_reply.started":"2025-06-24T12:24:03.776826Z","shell.execute_reply":"2025-06-24T12:24:03.788490Z"}},"outputs":[{"name":"stdout","text":"Predicted Tokens:\n\n\t create\n\t create\n\t create\n\t create\n\t create\n\t create\n\t create\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\npredicted_ids = predicted_id\nprint(predicted_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:03.936253Z","iopub.execute_input":"2025-06-24T12:24:03.936482Z","iopub.status.idle":"2025-06-24T12:24:03.941433Z","shell.execute_reply.started":"2025-06-24T12:24:03.936466Z","shell.execute_reply":"2025-06-24T12:24:03.940714Z"}},"outputs":[{"name":"stdout","text":"tensor([3])\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"trainer = L.Trainer(max_epochs=30)\ntrainer.fit(model, train_dataloaders=dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:04.099913Z","iopub.execute_input":"2025-06-24T12:24:04.100384Z","iopub.status.idle":"2025-06-24T12:24:04.532809Z","shell.execute_reply.started":"2025-06-24T12:24:04.100363Z","shell.execute_reply":"2025-06-24T12:24:04.532145Z"}},"outputs":[{"name":"stderr","text":"INFO: ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name                 | Type               | Params | Mode \n--------------------------------------------------------------------\n0 | we                   | Embedding          | 16     | train\n1 | pe                   | PositionalEncoding | 0      | train\n2 | self_attention       | Attention          | 12     | train\n3 | self_attention_2     | Attention          | 12     | train\n4 | self_attention_3     | Attention          | 12     | train\n5 | self_attention_4     | Attention          | 12     | train\n6 | reduce_attention_dim | Linear             | 18     | train\n7 | fc_layer             | Linear             | 24     | train\n8 | loss                 | CrossEntropyLoss   | 0      | train\n--------------------------------------------------------------------\n106       Trainable params\n0         Non-trainable params\n106       Total params\n0.000     Total estimated model params size (MB)\n21        Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64854b8602d64abd80253b158511a55b"}},"metadata":{}},{"name":"stdout","text":"Loss: 2.132384777069092\nLoss: 1.8171823024749756\nLoss: 1.5985180139541626\nLoss: 1.4378249645233154\nLoss: 1.3075172901153564\nLoss: 1.1973940134048462\nLoss: 1.1041547060012817\nLoss: 1.0170232057571411\nLoss: 0.9212038516998291\nLoss: 0.8178309202194214\nLoss: 0.7340945601463318\nLoss: 0.6448743939399719\nLoss: 0.5580506324768066\nLoss: 0.4920317232608795\nLoss: 0.4466918110847473\nLoss: 0.4077514410018921\nLoss: 0.36753010749816895\n","output_type":"stream"},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.32929039001464844\nLoss: 0.28941813111305237\nLoss: 0.2590929865837097\nLoss: 0.2363891750574112\nLoss: 0.21618640422821045\nLoss: 0.20074868202209473\nLoss: 0.18827491998672485\nLoss: 0.17244865000247955\nLoss: 0.15360501408576965\nLoss: 0.13558350503444672\nLoss: 0.11649902164936066\nLoss: 0.09833530336618423\nLoss: 0.0826854482293129\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"\nmodel_input = torch.tensor([token_to_id[\"what\"], \n                            token_to_id[\"you\"], \n                            # token_to_id[\"statquest\"], \n                            # token_to_id[\"<EOS>\"]\n                           ])\ninput_length = model_input.size(dim=0)\n\npredictions = model(model_input) \npredicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\npredicted_ids = predicted_id\n\nfor i in range(input_length, max_length):\n    # if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n    #     break\n    \n    model_input = torch.cat((model_input, predicted_id))\n    \n    predictions = model(model_input) \n    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n    predicted_ids = torch.cat((predicted_ids, predicted_id))\n        \nprint(\"Predicted Tokens:\\n\") \nfor id in predicted_ids: \n    print(\"\\t\", id_to_token[id.item()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:24:10.331870Z","iopub.execute_input":"2025-06-24T12:24:10.332393Z","iopub.status.idle":"2025-06-24T12:24:10.342814Z","shell.execute_reply.started":"2025-06-24T12:24:10.332372Z","shell.execute_reply":"2025-06-24T12:24:10.342113Z"}},"outputs":[{"name":"stdout","text":"Predicted Tokens:\n\n\t cannot\n\t create\n\t you\n\t do\n\t not\n\t understand\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}